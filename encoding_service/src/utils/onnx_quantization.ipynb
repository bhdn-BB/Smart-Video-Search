{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-17T14:37:01.072108Z",
     "start_time": "2025-08-17T14:36:48.652946Z"
    }
   },
   "source": [
    "from encoding_service.src.global_configs import CLIP_IMAGE1_ONNX_PATH, CLIP_TEXT1_ONNX_PATH, DEVICE, CLIP_TEXT1_ONNX_PATH_FP16, CLIP_IMAGE1_ONNX_PATH_FP16\n",
    "from encoding_service.src.models.clip_openai.config import MODEL_CLIP\n",
    "import torch\n",
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from encoding_service.src.models.clip_openai.encoder_image.clip_image_encoder import CLIPImageEncoder\n",
    "from encoding_service.src.models.clip_openai.encoder_text.clip_text_encoder import CLIPTextEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "model = CLIPModel.from_pretrained(MODEL_CLIP).to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_CLIP)\n",
    "\n",
    "text_encoder = CLIPTextEncoder(model).to(DEVICE)\n",
    "image_encoder = CLIPImageEncoder(model).to(DEVICE)\n",
    "\n",
    "model.eval()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:26:27.242189Z",
     "start_time": "2025-08-17T14:26:21.509629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#============================Text===============================================\n",
    "def convert_text():\n",
    "    try:\n",
    "        text_input = processor(\n",
    "            text=[\"hello world\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            torch.onnx.export(\n",
    "                text_encoder,\n",
    "                (text_input[\"input_ids\"], text_input[\"attention_mask\"]),\n",
    "                CLIP_TEXT1_ONNX_PATH,\n",
    "                do_constant_folding=True,\n",
    "                export_params=True,\n",
    "                input_names=[\"input_ids\", \"attention_mask\"],\n",
    "                output_names=[\"text_features\"],\n",
    "                dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch\", 1: \"seq_len\"},\n",
    "                    \"attention_mask\": {0: \"batch\", 1: \"seq_len\"},\n",
    "                    \"text_features\": {0: \"batch\"},\n",
    "                },\n",
    "                opset_version=17\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "convert_text()\n",
    "\n"
   ],
   "id": "d6a90fee016a32bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:237: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_length > max_position_embedding:\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:94: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:170: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:27:00.229889Z",
     "start_time": "2025-08-17T14:26:51.740601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = onnx.load(CLIP_TEXT1_ONNX_PATH)\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "onnx.save(model_fp16, CLIP_TEXT1_ONNX_PATH_FP16)"
   ],
   "id": "640b320b852adfce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnxconverter_common\\float16.py:63: UserWarning: the float32 number -3.4028234663852886e+38 will be truncated to -10000.0\n",
      "  warnings.warn(\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnxconverter_common\\float16.py:52: UserWarning: the float32 number 5.160036623246683e-14 will be truncated to 1e-07\n",
      "  warnings.warn(\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnxconverter_common\\float16.py:70: UserWarning: the float32 number -2.962422516496199e-14 will be truncated to -1e-07\n",
      "  warnings.warn(\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnxconverter_common\\float16.py:70: UserWarning: the float32 number -1.2970960305835888e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnxconverter_common\\float16.py:52: UserWarning: the float32 number 5.960464477539063e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\n",
      "C:\\Users\\Roma\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\onnxconverter_common\\float16.py:70: UserWarning: the float32 number -5.960464477539063e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:37:41.987055Z",
     "start_time": "2025-08-17T14:37:17.546039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#=======================Image===================================================\n",
    "def convert_image():\n",
    "    try:\n",
    "        image = torch.rand(1, 3, 224, 224, device=DEVICE)\n",
    "        image_input = processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            torch.onnx.export(\n",
    "                image_encoder,\n",
    "                (image_input,),\n",
    "                CLIP_IMAGE1_ONNX_PATH,\n",
    "                do_constant_folding=True,\n",
    "                export_params=True,\n",
    "                input_names=[\"pixel_values\"],\n",
    "                output_names=[\"image_features\"],\n",
    "                dynamic_axes={\n",
    "                    \"pixel_values\": {0: \"batch\"},\n",
    "                    \"image_features\": {0: \"batch\"},\n",
    "                },\n",
    "                opset_version=17\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "convert_image()\n"
   ],
   "id": "9ba39705662da82e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:39:00.928752Z",
     "start_time": "2025-08-17T14:38:27.879858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = onnx.load(CLIP_IMAGE1_ONNX_PATH)\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "onnx.save(model_fp16, CLIP_IMAGE1_ONNX_PATH_FP16)"
   ],
   "id": "7b32be451c15c7dd",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
