{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from encoding_service.src.global_configs import (\n",
    "    CLIP_IMAGE1_ONNX_PATH,\n",
    "    CLIP_TEXT1_ONNX_PATH,\n",
    "    DEVICE,\n",
    "    CLIP_TEXT1_ONNX_PATH_FP16,\n",
    "    CLIP_IMAGE1_ONNX_PATH_FP16,\n",
    ")\n",
    "from encoding_service.src.models.clip_openai.config import MODEL_CLIP\n",
    "import torch\n",
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from encoding_service.src.models.clip_openai.encoder_image.clip_image_encoder import CLIPImageEncoder\n",
    "from encoding_service.src.models.clip_openai.encoder_text.clip_text_encoder import CLIPTextEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "model = CLIPModel.from_pretrained(MODEL_CLIP).to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_CLIP)\n",
    "\n",
    "text_encoder = CLIPTextEncoder(model).to(DEVICE)\n",
    "image_encoder = CLIPImageEncoder(model).to(DEVICE)\n",
    "\n",
    "model.eval()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#============================Text===============================================\n",
    "def convert_text():\n",
    "    try:\n",
    "        text_input = processor(\n",
    "            text=[\"hello world\"],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            torch.onnx.export(\n",
    "                text_encoder,\n",
    "                (text_input[\"input_ids\"], text_input[\"attention_mask\"]),\n",
    "                CLIP_TEXT1_ONNX_PATH,\n",
    "                do_constant_folding=True,\n",
    "                export_params=True,\n",
    "                input_names=[\"input_ids\", \"attention_mask\"],\n",
    "                output_names=[\"text_features\"],\n",
    "                dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch\", 1: \"seq_len\"},\n",
    "                    \"attention_mask\": {0: \"batch\", 1: \"seq_len\"},\n",
    "                    \"text_features\": {0: \"batch\"},\n",
    "                },\n",
    "                opset_version=17\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "convert_text()\n",
    "\n"
   ],
   "id": "d6a90fee016a32bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = onnx.load(CLIP_TEXT1_ONNX_PATH)\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "onnx.save(model_fp16, CLIP_TEXT1_ONNX_PATH_FP16)"
   ],
   "id": "640b320b852adfce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#=======================Image===================================================\n",
    "def convert_image():\n",
    "    try:\n",
    "        image = torch.rand(1, 3, 224, 224, device=DEVICE)\n",
    "        image_input = processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            torch.onnx.export(\n",
    "                image_encoder,\n",
    "                (image_input,),\n",
    "                CLIP_IMAGE1_ONNX_PATH,\n",
    "                do_constant_folding=True,\n",
    "                export_params=True,\n",
    "                input_names=[\"pixel_values\"],\n",
    "                output_names=[\"image_features\"],\n",
    "                dynamic_axes={\n",
    "                    \"pixel_values\": {0: \"batch\"},\n",
    "                    \"image_features\": {0: \"batch\"},\n",
    "                },\n",
    "                opset_version=17\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "convert_image()\n"
   ],
   "id": "9ba39705662da82e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = onnx.load(CLIP_IMAGE1_ONNX_PATH)\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "onnx.save(model_fp16, CLIP_IMAGE1_ONNX_PATH_FP16)"
   ],
   "id": "7b32be451c15c7dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc930c6e4697cb00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
